# DeepRL-MLP-Snake ğŸ

*English | [ä¸­æ–‡](#ä¸­æ–‡è¯´æ˜)*

A high-performance Deep Reinforcement Learning Snake game using Deep Q-Networks (DQN) with advanced optimization techniques.

## ğŸš€ Features

- **Deep Q-Learning**: Implementation of DQN with experience replay and target networks
- **Priority Experience Replay**: Uses SumTree for efficient prioritized sampling
- **Numba Acceleration**: JIT-compiled critical functions for 10x performance boost
- **Advanced State Representation**: 11-dimensional state space including danger detection, food direction, and local grid view
- **Anti-suicide Mechanism**: Prevents the snake from making obviously dangerous moves during training
- **Real-time Visualization**: Live training progress with matplotlib plots
- **Model Checkpointing**: Automatic saving of best models and periodic backups

## ğŸ“Š Performance Metrics

- **Training Speed**: ~1000 episodes/hour on modern GPU
- **State Space**: Optimized feature vector (removed full grid)
- **Action Space**: 3 discrete actions (straight, turn right, turn left)
- **Memory Capacity**: 200K transitions with prioritized replay
- **Network Architecture**: Efficient MLP with [128, 64, 32, 16, 8] hidden layers

## ğŸ› ï¸ Installation

### Prerequisites
- Python 3.11 or higher
- CUDA-compatible GPU (optional but recommended)

### Setup
```bash
# Clone the repository
git clone https://github.com/your-username/DeepRL-MLP-Snake.git
cd DeepRL-MLP-Snake

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

## ğŸ¯ Quick Start

### Basic Training
```bash
# Start training with visualization
python train.py

# Train without visualization (faster)
python train.py --no-visual

# Train for specific episodes
python train.py --episodes 5000
```

### Advanced Training Options
```python
from train import train

# Custom training
train(
    num_episodes=10000,
    visualize=True,
    verbose=True
)
```

## ğŸ“ Project Structure

```
DeepRL-MLP-Snake/
â”œâ”€â”€ agent.py              # DQN Agent implementation
â”œâ”€â”€ config.py             # Configuration parameters
â”œâ”€â”€ game.py               # Snake game environment
â”œâ”€â”€ model.py              # Neural network architecture
â”œâ”€â”€ memory.py             # Prioritized experience replay
â”œâ”€â”€ train.py              # Training script
â”œâ”€â”€ training.py           # Visualization and stats
â”œâ”€â”€ help.py               # Numba-optimized helper functions
â”œâ”€â”€ direction.py          # Direction enum
â”œâ”€â”€ models/               # Saved model checkpoints
â”œâ”€â”€ requirements.txt      # Python dependencies
â””â”€â”€ README.md            # This file
```

## ğŸ”§ Configuration

Key parameters in `config.py`:

```python
# Network Architecture (Optimized)
HIDDEN_LAYERS = [128, 64, 32, 16, 8]  # More efficient design

# Training Parameters
BATCH_SIZE = 128                      # Reduced from 512
LEARNING_RATE = 3.48e-5
MEMORY_CAPACITY = 200_000            # Reduced from 2M
GAMMA = 0.99

# Exploration
EPS_START = 1.0
EPS_END = 0.02
EPS_DECAY = 7000

# Grid Configuration
GRID_WIDTH = 12                       # Increased from 11
GRID_HEIGHT = 12                      # Increased from 11
```

## ğŸ® Game Mechanics

### State Representation (Compact Design)
1. **Danger Detection**: 3 binary flags (front, right, left)
2. **Head Position**: Normalized coordinates
3. **Food Position**: Normalized coordinates
4. **Relative Distance**: Vector to food
5. **Manhattan Distance**: Steps to food
6. **Direction**: One-hot encoded current direction
7. **8-Direction Dangers**: Collision detection in 8 directions
8. **Boundary Distances**: Distance to walls
9. **Snake Length**: Normalized length
10. **Free Space Ratio**: Available space percentage
11. **Local Grid View**: 6x6 grid around head
12. **Action History**: Last 5 actions

*Note: Full grid representation has been removed for efficiency*

### Reward Structure
- **Food**: +16.0
- **Collision**: -10.0
- **Progress**: +0.1 (moving closer to food)
- **Step**: -0.01 (time penalty)

## ğŸ“ˆ Training Visualization

The training process displays:
- Real-time score progression
- Moving average scores (500-episode window)
- Total rewards per episode
- Training steps per episode
- Loss curves and exploration rate

## ğŸ”¬ Advanced Features

### Numba Acceleration
Critical game logic functions are JIT-compiled using Numba for maximum performance:
- Collision detection
- Distance calculations
- Experience replay sampling
- Game step logic

### Priority Experience Replay
- Uses SumTree data structure for O(log n) sampling
- Prioritized by TD-error magnitude
- Importance sampling weights for bias correction

### Anti-suicide Mechanism
During training, the agent avoids obviously dangerous moves even during exploration, significantly improving sample efficiency.

## ğŸ“Š Results

Typical training results after 1000 episodes:
- **Average Score**: 15-25
- **Peak Score**: 40-50
- **Training Time**: ~1 hour on RTX 3060

## ğŸ¤ Contributing

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- Inspired by DeepMind's DQN paper
- Built with PyTorch and Numba
- Game framework based on Pygame

---

## ä¸­æ–‡è¯´æ˜ ğŸ‡¨ğŸ‡³

## ğŸš€ åŠŸèƒ½ç‰¹æ€§

- **æ·±åº¦Qå­¦ä¹ **: å®ç°å¸¦æœ‰ç»éªŒå›æ”¾å’Œç›®æ ‡ç½‘ç»œçš„DQNç®—æ³•
- **ä¼˜å…ˆçº§ç»éªŒå›æ”¾**: ä½¿ç”¨SumTreeè¿›è¡Œé«˜æ•ˆä¼˜å…ˆçº§é‡‡æ ·
- **NumbaåŠ é€Ÿ**: JITç¼–è¯‘å…³é”®å‡½æ•°ï¼Œæ€§èƒ½æå‡10å€
- **é«˜çº§çŠ¶æ€è¡¨ç¤º**: 11ç»´çŠ¶æ€ç©ºé—´ï¼ŒåŒ…æ‹¬å±é™©æ£€æµ‹ã€é£Ÿç‰©æ–¹å‘å’Œå±€éƒ¨ç½‘æ ¼è§†å›¾
- **é˜²è‡ªæ€æœºåˆ¶**: è®­ç»ƒæœŸé—´é˜²æ­¢è›‡åšå‡ºæ˜æ˜¾å±é™©çš„åŠ¨ä½œ
- **å®æ—¶å¯è§†åŒ–**: ä½¿ç”¨matplotlibå®æ—¶æ˜¾ç¤ºè®­ç»ƒè¿›åº¦
- **æ¨¡å‹æ£€æŸ¥ç‚¹**: è‡ªåŠ¨ä¿å­˜æœ€ä½³æ¨¡å‹å’Œå®šæœŸå¤‡ä»½

## ğŸ“Š æ€§èƒ½æŒ‡æ ‡

- **è®­ç»ƒé€Ÿåº¦**: ç°ä»£GPUä¸Šçº¦1000å±€/å°æ—¶
- **çŠ¶æ€ç©ºé—´**: ä¼˜åŒ–åçš„ç‰¹å¾å‘é‡ï¼ˆç§»é™¤å®Œæ•´ç½‘æ ¼ï¼‰
- **åŠ¨ä½œç©ºé—´**: 3ä¸ªç¦»æ•£åŠ¨ä½œï¼ˆç›´è¡Œã€å³è½¬ã€å·¦è½¬ï¼‰
- **è®°å¿†å®¹é‡**: 20ä¸‡æ¡è½¬æ¢è®°å½•ï¼Œå¸¦ä¼˜å…ˆçº§å›æ”¾
- **ç½‘ç»œæ¶æ„**: é«˜æ•ˆMLPï¼Œéšè—å±‚[128, 64, 32, 16, 8]

## ğŸ› ï¸ å®‰è£…æŒ‡å—

### ç³»ç»Ÿè¦æ±‚
- Python 3.11æˆ–æ›´é«˜ç‰ˆæœ¬
- CUDAå…¼å®¹GPUï¼ˆå¯é€‰ä½†æ¨èï¼‰

### å®‰è£…æ­¥éª¤
```bash
# å…‹éš†ä»“åº“
git clone https://github.com/your-username/DeepRL-MLP-Snake.git
cd DeepRL-MLP-Snake

# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# å®‰è£…ä¾èµ–
pip install -r requirements.txt
```

## ğŸ¯ å¿«é€Ÿå¼€å§‹

### åŸºç¡€è®­ç»ƒ
```bash
# å¼€å§‹å¸¦å¯è§†åŒ–çš„è®­ç»ƒ
python train.py

# æ— å¯è§†åŒ–è®­ç»ƒï¼ˆæ›´å¿«ï¼‰
python train.py --no-visual

# è®­ç»ƒæŒ‡å®šå±€æ•°
python train.py --episodes 5000
```

### é«˜çº§è®­ç»ƒé€‰é¡¹
```python
from train import train

# è‡ªå®šä¹‰è®­ç»ƒ
train(
    num_episodes=10000,
    visualize=True,
    verbose=True
)
```

## ğŸ“ é¡¹ç›®ç»“æ„

```
DeepRL-MLP-Snake/
â”œâ”€â”€ agent.py              # DQNæ™ºèƒ½ä½“å®ç°
â”œâ”€â”€ config.py             # é…ç½®å‚æ•°
â”œâ”€â”€ game.py               # è´ªåƒè›‡æ¸¸æˆç¯å¢ƒ
â”œâ”€â”€ model.py              # ç¥ç»ç½‘ç»œæ¶æ„
â”œâ”€â”€ memory.py             # ä¼˜å…ˆçº§ç»éªŒå›æ”¾
â”œâ”€â”€ train.py              # è®­ç»ƒè„šæœ¬
â”œâ”€â”€ training.py           # å¯è§†åŒ–å’Œç»Ÿè®¡
â”œâ”€â”€ help.py               # Numbaä¼˜åŒ–çš„è¾…åŠ©å‡½æ•°
â”œâ”€â”€ direction.py          # æ–¹å‘æšä¸¾
â”œâ”€â”€ models/               # ä¿å­˜çš„æ¨¡å‹æ£€æŸ¥ç‚¹
â”œâ”€â”€ requirements.txt      # Pythonä¾èµ–
â””â”€â”€ README.md            # æœ¬æ–‡ä»¶
```

## ğŸ”§ é…ç½®å‚æ•°

`config.py`ä¸­çš„å…³é”®å‚æ•°ï¼š

```python
# ç½‘ç»œæ¶æ„
HIDDEN_LAYERS = [1024, 512, 256, 128, 64, 32, 16, 8]

# è®­ç»ƒå‚æ•°
BATCH_SIZE = 512
LEARNING_RATE = 3.48e-5
MEMORY_CAPACITY = 2_000_000
GAMMA = 0.99

# æ¢ç´¢å‚æ•°
EPS_START = 1.0
EPS_END = 0.02
EPS_DECAY = 7000
```

## ğŸ® æ¸¸æˆæœºåˆ¶

### çŠ¶æ€è¡¨ç¤ºï¼ˆç´§å‡‘è®¾è®¡ï¼‰
1. **å±é™©æ£€æµ‹**: 3ä¸ªäºŒè¿›åˆ¶æ ‡å¿—ï¼ˆå‰æ–¹ã€å³æ–¹ã€å·¦æ–¹ï¼‰
2. **å¤´éƒ¨ä½ç½®**: å½’ä¸€åŒ–åæ ‡
3. **é£Ÿç‰©ä½ç½®**: å½’ä¸€åŒ–åæ ‡
4. **ç›¸å¯¹è·ç¦»**: åˆ°é£Ÿç‰©çš„å‘é‡
5. **æ›¼å“ˆé¡¿è·ç¦»**: åˆ°é£Ÿç‰©çš„æ­¥æ•°
6. **æ–¹å‘**: å½“å‰æ–¹å‘çš„ä¸€çƒ­ç¼–ç 
7. **8æ–¹å‘å±é™©**: 8ä¸ªæ–¹å‘çš„ç¢°æ’æ£€æµ‹
8. **è¾¹ç•Œè·ç¦»**: åˆ°å¢™å£çš„è·ç¦»
9. **è›‡é•¿åº¦**: å½’ä¸€åŒ–é•¿åº¦
10. **ç©ºé—²ç©ºé—´æ¯”ä¾‹**: å¯ç”¨ç©ºé—´ç™¾åˆ†æ¯”
11. **å±€éƒ¨ç½‘æ ¼è§†å›¾**: å¤´éƒ¨å‘¨å›´6x6ç½‘æ ¼
12. **åŠ¨ä½œå†å²**: æœ€è¿‘5ä¸ªåŠ¨ä½œ

*æ³¨æ„ï¼šå®Œæ•´ç½‘æ ¼è¡¨ç¤ºå·²ç§»é™¤ä»¥æé«˜æ•ˆç‡*

### å¥–åŠ±ç»“æ„
- **é£Ÿç‰©**: +16.0
- **ç¢°æ’**: -10.0
- **è¿›åº¦**: +0.1ï¼ˆå‘é£Ÿç‰©ç§»åŠ¨ï¼‰
- **æ­¥æ•°**: -0.01ï¼ˆæ—¶é—´æƒ©ç½šï¼‰

## ğŸ“ˆ è®­ç»ƒå¯è§†åŒ–

è®­ç»ƒè¿‡ç¨‹æ˜¾ç¤ºï¼š
- å®æ—¶åˆ†æ•°è¿›å±•
- ç§»åŠ¨å¹³å‡åˆ†æ•°ï¼ˆ500å±€çª—å£ï¼‰
- æ¯å±€æ€»å¥–åŠ±
- æ¯å±€è®­ç»ƒæ­¥æ•°
- æŸå¤±æ›²çº¿å’Œæ¢ç´¢ç‡

## ğŸ”¬ é«˜çº§ç‰¹æ€§

### NumbaåŠ é€Ÿ
ä½¿ç”¨Numba JITç¼–è¯‘å…³é”®æ¸¸æˆé€»è¾‘å‡½æ•°ï¼Œè·å¾—æœ€å¤§æ€§èƒ½ï¼š
- ç¢°æ’æ£€æµ‹
- è·ç¦»è®¡ç®—
- ç»éªŒå›æ”¾é‡‡æ ·
- æ¸¸æˆæ­¥éª¤é€»è¾‘

### ä¼˜å…ˆçº§ç»éªŒå›æ”¾
- ä½¿ç”¨SumTreeæ•°æ®ç»“æ„ï¼ŒO(log n)é‡‡æ ·
- æŒ‰TDè¯¯å·®å¹…åº¦ä¼˜å…ˆçº§æ’åº
- é‡è¦æ€§é‡‡æ ·æƒé‡ç”¨äºåå·®æ ¡æ­£

### é˜²è‡ªæ€æœºåˆ¶
è®­ç»ƒæœŸé—´ï¼Œæ™ºèƒ½ä½“å³ä½¿åœ¨æ¢ç´¢æ—¶ä¹Ÿé¿å…æ˜æ˜¾å±é™©çš„åŠ¨ä½œï¼Œæ˜¾è‘—æé«˜æ ·æœ¬æ•ˆç‡ã€‚

## ğŸ“Š è®­ç»ƒç»“æœ

1000å±€åçš„å…¸å‹è®­ç»ƒç»“æœï¼š
- **å¹³å‡åˆ†æ•°**: 15-25
- **æœ€é«˜åˆ†æ•°**: 40-50
- **è®­ç»ƒæ—¶é—´**: RTX 3060ä¸Šçº¦1å°æ—¶

## ğŸ¤ è´¡çŒ®æŒ‡å—

1. Forkæœ¬ä»“åº“
2. åˆ›å»ºåŠŸèƒ½åˆ†æ”¯ (`git checkout -b feature/AmazingFeature`)
3. æäº¤æ›´æ”¹ (`git commit -m 'Add some AmazingFeature'`)
4. æ¨é€åˆ°åˆ†æ”¯ (`git push origin feature/AmazingFeature`)
5. å¼€å¯Pull Request

## ğŸ“ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨MITè®¸å¯è¯ - æŸ¥çœ‹[LICENSE](LICENSE)æ–‡ä»¶äº†è§£è¯¦æƒ…ã€‚

## ğŸ™ è‡´è°¢

- çµæ„Ÿæ¥è‡ªDeepMindçš„DQNè®ºæ–‡
- ä½¿ç”¨PyTorchå’ŒNumbaæ„å»º
- æ¸¸æˆæ¡†æ¶åŸºäºPygame